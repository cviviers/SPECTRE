import math

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist


class SigLIPLossOld(nn.Module):
    def __init__(
        self, 
        learnable_t: bool = True, 
        learnable_b: bool = True, 
        normalize: bool = True,
        init_t: float = math.log(10),  # Default temperature for SigLIP
        init_b: float = -10.0,  # Default bias for SigLIP
    ):
        """
        SigLip loss for aligning image and text embeddings.

        Args:
            learnable_t (bool): If True, temperature `t` is a learnable parameter.
            learnable_b (bool): If True, bias `b` is a learnable parameter.
            normalize (bool): If True, embeddings are L2-normalized before computing logits.
            init_t (float): Initial value for temperature.
            init_b (float): Initial value for bias.
        """
        super().__init__()
        self.normalize = normalize

        # Define learnable parameters for temperature and bias
        self.t = nn.Parameter(torch.tensor(init_t)) if learnable_t else init_t
        self.b = nn.Parameter(torch.tensor(init_b)) if learnable_b else init_b

    def forward(
        self, 
        zimg: torch.Tensor, 
        ztxt: torch.Tensor,
        return_details: bool = False
    ) -> torch.Tensor:
        """
        Computes the alignment loss between image and text embeddings.

        Args:
            zimg (torch.Tensor): Image embeddings of shape (batch_size, embedding_dim).
            ztxt (torch.Tensor): Text embeddings of shape (batch_size, embedding_dim).

        Returns:
            torch.Tensor: Computed loss value.
        """
        if self.normalize:
            zimg = F.normalize(zimg, p=2, dim=-1)  # (batch_size, embedding_dim)
            ztxt = F.normalize(ztxt, p=2, dim=-1)  # (batch_size, embedding_dim)

        logits = torch.matmul(zimg, ztxt.T)  # Compute similarity matrix (batch_size, batch_size)
        logits = logits * self.t + self.b  # Apply scaling and bias

        batch_size = logits.size(0)
        eye = torch.eye(batch_size, device=logits.device)  # Identity matrix for positives

        m1_diag1 = -torch.ones_like(logits, device=logits.device) + 2 * eye
        loglik = F.logsigmoid(m1_diag1 * logits)  # Log sigmoid  (batch_size, batch_size)
        nll = -torch.sum(loglik, dim=-1)  # Negative log likelihood
        loss = torch.mean(nll)  # Final mean loss

        if not return_details:
            return loss
        
        pos_loglik = torch.diag(loglik)
        pos_loss = -pos_loglik.sum() / batch_size  # Contribution of positive pairs

        neg_loglik = loglik * (1 - eye)  # Exclude diagonal for negatives
        neg_loss = -neg_loglik.sum() / batch_size  # Contribution of negative pairs

        return loss, {"pos_loss": pos_loss.item(), "neg_loss": neg_loss.item()}


class SigLIPLoss(nn.Module):
    def __init__(
        self, 
        learnable_t: bool = True, 
        learnable_b: bool = True, 
        normalize: bool = True,
        init_t: float = math.log(10),  # Default temperature for SigLIP
        init_b: float = -10.0,  # Default bias for SigLIP
    ):
        """
        SigLip loss for aligning image and text embeddings.

        Args:
            learnable_t (bool): If True, temperature `t` is a learnable parameter.
            learnable_b (bool): If True, bias `b` is a learnable parameter.
            normalize (bool): If True, embeddings are L2-normalized before computing logits.
            init_t (float): Initial value for temperature.
            init_b (float): Initial value for bias.
        """
        super().__init__()
        self.normalize = normalize

        # Define learnable parameters for temperature and bias
        self.t = nn.Parameter(torch.tensor(init_t)) if learnable_t else init_t
        self.b = nn.Parameter(torch.tensor(init_b)) if learnable_b else init_b
    
    @staticmethod
    def slice_loglik(logits: torch.Tensor, include_pos: bool) -> torch.Tensor:
        """
        Computes the log-likelihood for positive and negative pairs in the logits matrix.

        Args:
            logits (torch.Tensor): Logits matrix of shape (batch_size, batch_size).
            include_pos (bool): If True, includes positive pairs in the log-likelihood calculation.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Positive and negative log-likelihoods.
        """
        B = logits.size(0)

        if include_pos:
            # Positive pairs are on the diagonal
            pos_mask = torch.eye(B, device=logits.device)
        else:
            # If not including positive pairs, we treat all pairs as negatives
            pos_mask = torch.zeros(B, B, device=logits.device)
        neg_mask = 1.0 - pos_mask

        m1 = -torch.ones_like(logits, device=logits.device)
        m1 += 2 * pos_mask  # Add identity matrix to the diagonal

        # joint log-likelihood
        loglik = F.logsigmoid(m1 * logits)

        pos_ll = (loglik * pos_mask).sum(dim=-1)  # positive log likelihood
        neg_ll = (loglik * neg_mask).sum(dim=-1)  # negative log likelihood

        return pos_ll, neg_ll

    def forward(
        self, 
        zimg: torch.Tensor, 
        ztxt: torch.Tensor,
        return_details: bool = False
    ) -> torch.Tensor:
        """
        Computes the alignment loss between image and text embeddings.

        Args:
            zimg (torch.Tensor): Image embeddings of shape (batch_size, embedding_dim).
            ztxt (torch.Tensor): Text embeddings of shape (batch_size, embedding_dim).

        Returns:
            torch.Tensor: Computed loss value.
        """
        if self.normalize:
            print(f"[Rank {dist.get_rank() if dist.is_initialized() else 0}] Normalizing embeddings")
            zimg = F.normalize(zimg, p=2, dim=-1)
            ztxt = F.normalize(ztxt, p=2, dim=-1)

        # ---- setup distributed ----
        if not dist.is_initialized():
            # fallback to single-GPU
            print("[Single GPU] Computing logits")
            logits = zimg @ ztxt.t()
            logits = logits * self.t + self.b

            print("[Single GPU] Computing slice_loglik")
            pos_ll, neg_ll = self.slice_loglik(logits, include_pos=True)

            pos_loss = -pos_ll.mean()  # mean loss for positives
            neg_loss = -neg_ll.mean()  # mean loss for negatives

            loss = pos_loss + neg_loss  # total loss
            
            if return_details:
                print("[Single GPU] Returning loss with details")
                return loss, {"pos_loss": pos_loss.item(), "neg_loss": neg_loss.item()}
            print("[Single GPU] Returning loss")
            return loss

        world_size = dist.get_world_size()
        rank = dist.get_rank()
        B = zimg.size(0)

        # buffer for the rotating text embeddings
        # start by copying the local ztxt into it
        print(f"[Rank {rank}] Distributed mode: world_size={world_size}, batch_size={B}")
        ztxt_rot = ztxt.clone()

        # accumulators (sum of per-sample losses)
        pos_sum = torch.tensor(0., device=zimg.device)
        neg_sum = torch.tensor(0., device=zimg.device)
        samples_cnt = torch.tensor(0, device=zimg.device)

        for k in range(world_size):
            print(f"[Rank {rank}] Loop {k}/{world_size}")
            if k > 0:
                # this will overwrite ztxt_rot with the embeddings from rank=src
                src = (rank + k) % world_size
                print(f"[Rank {rank}] Broadcasting ztxt_rot from src={src}")
                dist.broadcast(ztxt_rot, src=src)
                print(f"[Rank {rank}] Finished broadcast for k={k}")

            print(f"[Rank {rank}] Computing logits for k={k}")
            # now compute this “slice” of the full N×N logits:
            logits = zimg @ ztxt_rot.t()  # (batch_size, batch_size)
            logits = logits * self.t + self.b

            if k == 0:
                print(f"[Rank {rank}] Computing slice_loglik with positives for k={k}")
                pos_ll, neg_ll = self.slice_loglik(logits, include_pos=True)
                pos_sum += pos_ll.sum()  # accumulate positive log likelihood
                neg_sum += neg_ll.sum()  # accumulate negative log likelihood

            else:
                # for all other slices, we only compute the negative log likelihood
                # since the positive pairs are already included in the first slice
                print(f"[Rank {rank}] Computing slice_loglik without positives for k={k}")
                pos_ll, neg_ll = self.slice_loglik(logits, include_pos=False)
                neg_sum += neg_ll.sum()

            samples_cnt += B  # accumulate the number of samples processed
            print(f"[Rank {rank}] samples_cnt={samples_cnt.item()} after k={k}")

        print(f"[Rank {rank}] Calling dist.all_reduce for pos_sum, neg_sum, samples_cnt")
        dist.all_reduce(pos_sum, op=dist.ReduceOp.SUM)
        dist.all_reduce(neg_sum, op=dist.ReduceOp.SUM)
        dist.all_reduce(samples_cnt, op=dist.ReduceOp.SUM)
        print(f"[Rank {rank}] Finished all_reduce")

        # Compute the final loss
        pos_loss = -pos_sum / samples_cnt
        neg_loss = -neg_sum / samples_cnt

        # `total_loss` is now the total SigLIP loss summed over all examples and all ranks
        total_loss = pos_loss + neg_loss

        if return_details:
            print(f"[Rank {rank}] Returning total_loss with details")
            return total_loss, {"pos_loss": pos_loss.item(), "neg_loss": neg_loss.item()}
        print(f"[Rank {rank}] Returning total_loss")
        return total_loss
