import math

import torch
import torch.nn as nn
import torch.nn.functional as F


class SigLIPLoss(nn.Module):
    def __init__(
        self, 
        learnable_t: bool = True, 
        learnable_b: bool = True, 
        normalize: bool = True,
        init_t: float = math.log(10),  # Default temperature for SigLIP
        init_b: float = -10.0,  # Default bias for SigLIP
    ):
        """
        SigLip loss for aligning image and text embeddings.

        Args:
            learnable_t (bool): If True, temperature `t` is a learnable parameter.
            learnable_b (bool): If True, bias `b` is a learnable parameter.
            normalize (bool): If True, embeddings are L2-normalized before computing logits.
            init_t (float): Initial value for temperature.
            init_b (float): Initial value for bias.
        """
        super().__init__()
        self.normalize = normalize

        # Define learnable parameters for temperature and bias
        self.t = nn.Parameter(torch.tensor(init_t)) if learnable_t else init_t
        self.b = nn.Parameter(torch.tensor(init_b)) if learnable_b else init_b

    def forward(self, zimg: torch.Tensor, ztxt: torch.Tensor) -> torch.Tensor:
        """
        Computes the alignment loss between image and text embeddings.

        Args:
            zimg (torch.Tensor): Image embeddings of shape (batch_size, embedding_dim).
            ztxt (torch.Tensor): Text embeddings of shape (batch_size, embedding_dim).

        Returns:
            torch.Tensor: Computed loss value.
        """
        if self.normalize:
            zimg = F.normalize(zimg, p=2, dim=-1)
            ztxt = F.normalize(ztxt, p=2, dim=-1)

        logits = torch.matmul(zimg, ztxt.T)  # Compute similarity matrix
        logits = logits * self.t + self.b  # Apply scaling and bias

        batch_size = logits.size(0)
        eye = torch.eye(batch_size, device=logits.device)  # Identity matrix for positives

        m1_diag1 = -torch.ones_like(logits, device=logits.device) + 2 * eye
        loglik = F.logsigmoid(m1_diag1 * logits)  # Log sigmoid
        nll = -torch.sum(loglik, dim=-1)  # Negative log likelihood
        loss = torch.mean(nll)  # Final mean loss

        # pos_loglik = F.logsigmoid(torch.diag(logits))  # Log sigmoid for positive pairs

        # neg_loglik = F.logsigmoid(-logits)  # Log sigmoid for negative pairs
        # neg_loglik.fill_diagonal_(0)  # Set diagonal to zero for negatives

        # neg_term = neg_loglik.sum() / (batch_size ** 2 - batch_size)  # Average over negative pairs
        
        # loss = -pos_loglik.mean() - neg_term  # Combine positive and negative terms

        return loss
