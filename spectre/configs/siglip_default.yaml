train:
  batch_size_per_gpu: 128
  grad_accum_steps: 1
  datasets:
  - ct_rate
  - inspect
  - merlin
  data_dir: /data/
  output_dir: .
  saveckp_freq: 20
  resume_ckp: true
  seed: 0
  num_workers: 10
  load_fp16: true
  pin_memory: true
  persistent_workers: true
  drop_last: true
  cache_dataset: true
  cache_dir: /data/cache/monai/
  use_gds: false
  log_wandb: true
  log_freq: 1
optim:
  epochs: 100
  base_lr: 0.0005
  lr: 0.  # will be set after applying scaling rule
  warmup_epochs: 10
  freeze_backbone_epochs: 10  # -1 means fully freeze backbone
  min_lr: 1.0e-06
  clip_grad_norm: 1.0
  scaling_rule: sqrt_wrt_1024
  adamw_beta1: 0.9
  adamw_beta2: 0.95
model:
  architecture: vit_base_patch16_128
  pretrained_weights: null
  feature_comb_embed_dim: 768
  feature_comb_num_layers: 4
  feature_comb_num_heads: 12
  text_tokenizer: Qwen/Qwen3-Embedding-0.6B
  text_encoder_weights: /data/checkpoints/Qwen/Qwen3-Embedding-0.6B/model.safetensors
  use_lora: true
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_keywords:
  - query
  - key
  - value
  projection_dim: 512
  learnable_t: true
  learnable_b: true
  init_t: 2.3  # ~log(10)
  init_b: -10.0
  normalize: true
