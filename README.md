# ğŸŒŸ <span style="color:red">SPECTRE</span>: <span style="color:red">S</span>elf-supervised <span style="color:red">P</span>r<span style="color:red">e</span>training for <span style="color:red">CT</span> <span style="color:red">R</span>epresentation <span style="color:red">E</span>xtraction

SPECTRE is a framework for training and evaluating **transformer-based foundation models for 3D Computed Tomography (CT) scans**, and provdes a set of pretrained CT foundation models open to the research community. By leveraging self-supervised learning, SPECTRE enables the extraction of rich and generalizable representations from medical imaging data, which can be fine-tuned for various downstream tasks such as segmentation, classification, and anomaly detection.

SPECTRE has been trained on a large cohort of **open-source CT scans** of the **human abdomen and thorax**, as well as **paired scans with radiology reports**, enabling it to learn rich representations that generalize across different datasets and clinical settings.

This repository provides tools for pretraining, fine-tuning, and evaluating these models, ensuring robust performance across different CT datasets.

## ğŸ“‚ Repository Contents

This repository is organized as follows:

- ğŸš€ **`spectre/`** â€“ Contains the core package, including:
  - Pretraining methods
  - Model architectures
  - Data transformations

- ğŸ› ï¸ **`spectre/configs/`** â€“ Stores configuration files for different training settings.

- ğŸ”¬ **`experiments/`** â€“ Includes Python scripts for running various pretraining and downstream experiments.

- ğŸ“„ **`requirements.txt`** â€“ Lists the dependencies required to run the project.

- ğŸ³ **`Dockerfile`** â€“ Defines the environment for running SPECTRE inside a container.

## âš™ï¸ Setting Up the Environment

To get up and running with SPECTRE, follow these steps:

1. Clone this repository to your local machine:
   ```bash
   git clone https://github.com/cclaess/SPECTRE.git
   cd SPECTRE
   ```

2. Create a `.env` file in the root directory of the project.

3. Add your [Weights & Biases](https://wandb.ai/) API key to the `.env` file:
   ```bash
   WANDB_API_KEY=<your_api_key>
   ```

4. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## âš¡ Performing Pretraining
...

## ğŸ§© Performing Downstream Training
...

## ğŸ§ª Testing Your Algorithms
...

## ğŸ³ Building and Using Docker

To facilitate deployment and reproducibility, SPECTRE can be run using **Docker**. This allows you to set up a fully functional environment without manually installing dependencies.

### **Building the Docker Image**
First, ensure you have **Docker** installed. Then, navigate to the repository and build the image:
```bash
docker build -t spectre-ct .
```

### **Running Experiments Inside Docker**
Once the image is built, you can start a container and execute scripts inside it. For example, to run a DINO pretraining experiment:
```bash
docker run --gpus all --rm -v $(pwd):/app spectre-ct python3 experiments/pretraining/pretrain_dino.py --config_file spectre/configs/dino_default.yaml --output_dir $(pwd)/outputs/pretraining/dino/
```
- `--gpus all` enables GPU acceleration if available.
- `--rm` removes the container after execution.
- `-v $(pwd):/app` mounts the current directory inside the container.
- `python3 experiments/pretraining/pretrain_dino.py --config_file spectre/configs/dino_default.yaml --output_dir $(pwd)/outputs/pretraining/dino/` runs the DINO pretraining script with the default configuration file and stores the models weights in an output folder.


## ğŸ“œ Citation
If you use SPECTRE in your research or wish to cite it, please use the following BibTeX entry:
```
@article{spectre2025,
  title={Citation will be added upon publication},
  author={},
  journal={},
  year={2025},
  url={https://github.com/cclaess/SPECTRE}
}
```

## ğŸ¤ Acknowledgements
This project builds upon prior work in self-supervised learning, medical imaging, and transformer-based representation learning. We acknowledge the **open-source CT datasets** and **research, code, and packages** that made this research possible, including:
- [**DINO**](https://arxiv.org/abs/2104.14294), [**DINOv2**](https://arxiv.org/abs/2304.07193), [**MAE**](https://arxiv.org/abs/2111.06377), & [**SigLIP**](https://arxiv.org/abs/2303.15343): Self-supervised vision and vision-language representation learning approaches that inspired this work.
- [**timm**](https://timm.fast.ai/) & [**lightly**](https://docs.lightly.ai/self-supervised-learning/): Python libraries providing 2D PyTorch models (timm) and self-supervised learning methods (lightly), from which we adapted parts of the code for 3D.
- [**CT-RATE & CT-CLIP**](https://arxiv.org/abs/2403.17834): A dataset of thoracic CT scans paired with radiology reports (CT-RATE) and a CT foundation model based on the CLIP framework trained on this dataset (CT-CLIP).
- [**MERLIN**](https://arxiv.org/abs/2406.06512): A dataset of abdominal CT scans paired with radiology reports and ICD10 codes, along with a vision-language CT foundation model trained on this dataset.
